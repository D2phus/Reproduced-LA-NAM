{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Additive Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from typing import Tuple\n",
    "from typing import Dict\n",
    "from typing import Sequence\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn import datasets # datasets is a sub-package. https://stackoverflow.com/questions/41467570/sklearn-doesnt-have-attribute-datasets\n",
    "\n",
    "\n",
    "from config import Config, defaults\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NAM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Neural Net\n",
    "There is a DNN-based sub net for each feature. To better fit jagged functions, the author proposes *exp-centered* (ExU) hidden units.\n",
    "#### Activation \n",
    "Two different hidden unit (activation) are compared: standard ReLU, and ExU. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ExU\n",
    "ExU hidden unit use an activation function $f$ to compute $h(x)$ given by \n",
    "$$\n",
    "h(x) =f(e^{\\omega}*(x-b))\n",
    "$$\n",
    "where $f$ can be any activation function ($\\text{ReLU-n}$, ReLU capped at n, is used here), $\\omega, b$ are the weight and bias parameters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExU(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_features: int = 1, \n",
    "        out_features: int = 1, # note: output features can be of different dimension to the input features\n",
    "        ) -> None: # type-check\n",
    "            \"\"\"\n",
    "            an ExU unit, return ReLU_n[exp(w)*(x-b)], where w and b are the weight and bias parameters, \n",
    "            and ReLU_n is ReLU function capped at n.\n",
    "            Args:\n",
    "            in_features: scalar, size of each input sample\n",
    "            out_feature: scalar, size of each output sample\n",
    "\n",
    "            \"\"\"\n",
    "            super(ExU, self).__init__()\n",
    "            self.in_features = in_features\n",
    "            self.out_features = out_features\n",
    "            self.weights = nn.Parameter(torch.Tensor(in_features, out_features))\n",
    "            self.bias = nn.Parameter(torch.Tensor(in_features)) # note; a bias for each feature\n",
    "            \n",
    "            self.initialize_parameters()  \n",
    "            \n",
    "    def ReLU_n(self, n, x):\n",
    "        \"\"\"\n",
    "        ReLU capped at n\n",
    "        \"\"\"\n",
    "        x = F.relu(x)\n",
    "        return torch.clamp(x, 0, n)\n",
    "    \n",
    "            # TODO: how about bias? - bias ~ N(0, 0.5^2)\n",
    "    def initialize_parameters(self)->None:\n",
    "        \"\"\"\n",
    "        Initializing the parameters, specifically:\n",
    "        - for weights: normal distribution N(x, 0,5²) with x in [3, 4]\n",
    "        - for bias: N(0, 0.5²)\n",
    "        \n",
    "        note that a variance of 0.5 is introduced in the article, but in fact a std of 0.5 is used here\n",
    "        \"\"\"     \n",
    "                # mean = torch.rand(in_features) + torch.ones(in_features)*3\n",
    "                # std = torch.sqrt(torch.ones(in_features) * 0.5)\n",
    "                # self.weights = torch.normal(mean=mean, std=std)\n",
    "                \n",
    "                # mean = np.random.rand(1) + 3 # a random x in [3, 4]\n",
    "        mean = 4.0 # note a fixed mean of 4 is used in source codes \n",
    "        torch.nn.init.trunc_normal_(self.weights, mean=mean, std=0.5) \n",
    "        torch.nn.init.trunc_normal_(self.bias, std=0.5) \n",
    "          \n",
    "            \n",
    "    def forward(self, \n",
    "               inputs: torch.Tensor, \n",
    "               n: int = 1\n",
    "               ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        inputs of shape (batch_size, in_features)\n",
    "        \n",
    "        \"\"\"\n",
    "        # note: matrix product!\n",
    "        # print(\"exp:\", exp_cofficient)\n",
    "        output = (inputs-self.bias).matmul(torch.exp(self.weights))\n",
    "        # print(\"output:\", output)\n",
    "        # ReLU-n\n",
    "        # relu_n = ReLU_n(n)\n",
    "        output = self.ReLU_n(n, output)\n",
    "        # print(\"relu: \", output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.8018, -1.1089, -0.5853,  1.1464,  1.2863])\n",
      "tensor([1., 1., 1.], grad_fn=<ClampBackward1>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/apps/anaconda-ci/fgci-centos7-anaconda/software/anaconda/2023-01/2eea7963/lib/python3.10/site-packages/torch/nn/init.py:176: UserWarning: mean is more than 2 std from [a, b] in nn.init.trunc_normal_. The distribution of values may be incorrect.\n",
      "  return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n"
     ]
    }
   ],
   "source": [
    "def test_ExU():\n",
    "    # TODO\n",
    "    n_inputs = 5 \n",
    "    n_outputs = 3\n",
    "    exu = ExU(in_features=n_inputs, out_features=n_outputs)\n",
    "    inputs = torch.randn(n_inputs)\n",
    "    print(inputs)\n",
    "    y = exu(inputs, 1)\n",
    "    print(y)\n",
    "test_ExU()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Standard ReLU\n",
    "Standard linear ReLU is used as baseline to ExU, $h(x)$ is given by \n",
    "$$\n",
    "h(x) = f(\\omega *(x - b))\n",
    "$$\n",
    "where $f(x)$ \n",
    "$$\n",
    "f(x) = \\begin{cases} \n",
    "0, & x \\leq 0, \\\\\n",
    "x, & x>0\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearReLU(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_features: int = 1, \n",
    "        out_features: int = 1, # note: output features can be of different dimension to the input features\n",
    "        ) -> None: # type-check\n",
    "            \"\"\"\n",
    "            Standard linear ReLU hidden unit.\n",
    "            in_features: scalar, the size of input sample\n",
    "            out_feature: scalar, the size of output sample\n",
    "            \"\"\"\n",
    "            super(LinearReLU, self).__init__()\n",
    "            self.in_features = in_features\n",
    "            self.out_features = out_features\n",
    "            self.weights = nn.Parameter(torch.Tensor(in_features, out_features))\n",
    "            self.bias = nn.Parameter(torch.Tensor(in_features)) # note; a bias for each feature\n",
    "            \n",
    "            self.initialize_parameters()  \n",
    "            \n",
    "            # TODO: how about bias? - bias ~ N(0, 0.5^2)\n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"\n",
    "        Initializing the parameters. \n",
    "        - weights: regular, xavier uniform(why uniform?)\n",
    "        - bias: N(0, 0.5²)\n",
    "        \"\"\"\n",
    "        nn.init.xavier_uniform_(self.weights)\n",
    "        torch.nn.init.trunc_normal_(self.bias, std=0.5) # note \n",
    "          \n",
    "            \n",
    "    def forward(self, \n",
    "               inputs: torch.Tensor, \n",
    "               ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        inputs of shape (batch_size, in_features)\n",
    "        \"\"\"\n",
    "        output = torch.matmul((inputs-self.bias), self.weights)\n",
    "        output = F.relu(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.2031, -0.5871,  0.2684,  1.3680, -0.2177])\n",
      "tensor([0.0000, 0.0000, 1.0103], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def test_LinearReLU():\n",
    "    # TODO\n",
    "    n_inputs = 5 \n",
    "    n_outputs = 3\n",
    "    linearRelu = LinearReLU(in_features=n_inputs, out_features=n_outputs)\n",
    "    inputs = torch.randn(n_inputs)\n",
    "    print(inputs)\n",
    "    y = linearRelu(inputs)\n",
    "    print(y)\n",
    "test_LinearReLU()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature nets in NAMs are selected amongst \n",
    "\n",
    "(1) single hidden layer with standard ReLU units + 3 regular hidden layers, \n",
    "\n",
    "(2) single hidden layer with ExU units + 3 regular hidden layers.\n",
    "\n",
    "Additionally, ExUs are regularized by adding dropout to the end of each hidden layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureNN(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        config, \n",
    "        name: str, \n",
    "        in_features: int,\n",
    "        num_units: int, \n",
    "        feature_index: int, # to indicate which feature is learn in this subnet\n",
    "        ) -> None: # type-check\n",
    "            \"\"\"\n",
    "            subset for each feature\n",
    "            note page 6, \"Feature nets in NAMs are **selected** amongst (1) DNNs containing 3 hidden layers, \n",
    "            and (2) single hidden layer NNs with 1024 ExU units and ReLU-1 activation\" \n",
    "            \n",
    "            => (1) standard ReLU + 3 hidden layers, (2) ExU + 3 hidden layers\n",
    "            \n",
    "            Args:\n",
    "            in_features: scalar, size of each input sample \n",
    "            num_units: scalar, number of ExU/LinearReLU hidden units in the single hidden layer \n",
    "            \"\"\"\n",
    "            super(FeatureNN, self).__init__()\n",
    "            \n",
    "            self.config = config\n",
    "            self.in_features = in_features\n",
    "            self.num_units = num_units\n",
    "            self.feature_index = feature_index\n",
    "            \n",
    "            hidden_sizes = [self.num_units] + self.config.hidden_sizes\n",
    "            self.dropout = nn.Dropout(p=self.config.dropout)\n",
    "            \n",
    "            # refer to figure 2: a single hidden layer neural network with 1024 (a) standard ReLU, and (b) ExU\n",
    "            # regularization 1: dropout to regularzie ExUs in each feature net. \n",
    "            layers = []\n",
    "            if self.config.activation == \"exu\":\n",
    "                layers.append(ExU(in_features=self.in_features, out_features=self.num_units))\n",
    "            else:\n",
    "                layers.append(LinearReLU(in_features=self.in_features, out_features=self.num_units))\n",
    "            # layers.append(self.dropout)\n",
    "            \n",
    "            # some hidden layers are added though \n",
    "            # TODO: we use the same hidden layer configuration for both feature NN, and DNN baseline. \n",
    "            # The feature NN, however, has one additional layer, i.e. the one with ExU units. \n",
    "            # is it fair?\n",
    "            for in_f, out_f in zip(hidden_sizes[:], hidden_sizes[1:]):\n",
    "                layers.append(LinearReLU(in_features=in_f, out_features=out_f))\n",
    "               # layers.append(self.dropout)\n",
    "                \n",
    "            layers.append(nn.Linear(in_features=hidden_sizes[-1], out_features=1)) # last linear layer; out_features=1\n",
    "            # layers.append(self.dropout)\n",
    "            \n",
    "            # self.model = nn.ModuleList(layers) \n",
    "            self.model = nn.Sequential(*layers)\n",
    "            \n",
    "    def forward(self, inputs) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args: \n",
    "        inputs of shape (batch_size)?: feature TODO  \n",
    "        Return of shape (batch_size, out_features) = (batch_size, 1)\n",
    "        \n",
    "        \"\"\"\n",
    "        outputs = inputs.unsqueeze(1) # TODO: of shape (batch_size, 1)?\n",
    "        for layer in self.model:\n",
    "            outputs = self.dropout(layer(outputs))\n",
    "        return outputs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Standard ReLU vs. ExU: Accurately Fitting the Toy Dataset \n",
    "### Toy dataset \n",
    "A toy dataset is constructed as follow: for the input $x$, we sample 100 evenly spaced points in $[-1, 1]$. For each $x$, we sample $p$ uniformly random in $[0.1, 0.9)$ and generate 100 labels from a Bernoulli random variable which takes the value 1 with probability $p$.This creates a binary classfication dataset of $(x, y)$ tuples with 10,000 points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split \n",
    "class ToyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,\n",
    "                 config, \n",
    "                 N: int, \n",
    "                 x_start: float, \n",
    "                 x_end: float, \n",
    "                 p_start: float, \n",
    "                 p_end: float) -> None:\n",
    "        \"\"\"\n",
    "        N x N samples are generated.\n",
    "        Args:\n",
    "        N: the number of points \n",
    "        [x_start, x_end]: the range for point sampling\n",
    "        [p_start, p_end): the range for probabilities \n",
    "        \"\"\"\n",
    "        super(ToyDataset, self).__init__()\n",
    "        self.config = config\n",
    "        self.N = N\n",
    "        self.point = torch.linspace(start=x_start, end=x_end, steps=N).float()\n",
    "        self.X = torch.stack([self.point]*N, 1).flatten()\n",
    "        self.odds = (torch.rand(N)*(p_end-p_start) + p_start).float()\n",
    "        self.log_odds = torch.log(self.odds/(1-self.odds))\n",
    "        odds_cat = torch.stack([self.odds]*N, 1).flatten()\n",
    "        self.y = torch.bernoulli(odds_cat)\n",
    "        \n",
    "        # self.point = np.linspace(x_start, x_end, num=self.N, endpoint=False)\n",
    "        # self.X = torch.from_numpy(np.stack([self.point]*self.N, axis=1).flatten().float()\n",
    "        # self.odds = torch.from_numpy(np.random.uniform(p_start, p_end, self.N)).float()\n",
    "        # self.log_odds = np.log(self.odds/(1-self.odds))\n",
    "        # self.y = np.random.binomial(1, self.odds, (self.N, self.N)).flatten()\n",
    "        \n",
    "        self.setup_dataloaders() \n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[np.array, ...]:\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "    def plot_log_odds(self):\n",
    "        plt.plot(self.point, self.log_odds, 'o')\n",
    "        \n",
    "    def setup_dataloaders(self, val_split: float = 0.1, test_split: float = 0.2) -> Tuple[DataLoader, ...]: \n",
    "        \"\"\"\n",
    "        split the dataset into training set, validation set, and test set\n",
    "        \"\"\"\n",
    "        test_size = int(test_split * len(self))\n",
    "        val_size = int(val_split * len(self))\n",
    "        train_size = len(self) - test_size - val_size\n",
    "        \n",
    "        train_subset, val_subset, test_subset = random_split(self, [train_size, val_size, test_size])\n",
    "        \n",
    "        self.train_dl = DataLoader(train_subset, batch_size=self.config.batch_size, shuffle=True)\n",
    "        self.val_dl = DataLoader(val_subset, batch_size=self.config.batch_size, shuffle=False)\n",
    "        self.test_dl = DataLoader(test_subset, batch_size=self.config.batch_size, shuffle=False)\n",
    "    \n",
    "    def get_dataloaders(self) -> Tuple[DataLoader, ...]: \n",
    "        return self.train_dl, self.val_dl, self.test_dl\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def test_toy_dataset():\n",
    "    cfg = defaults()\n",
    "    N = 100\n",
    "    x_start, x_end = -1, 1\n",
    "    p_start, p_end = 0.1, 0.9\n",
    "    toy_dataset = ToyDataset(cfg, N, x_start, x_end, p_start, p_end)\n",
    "    toy_dataset.plot_log_odds()\n",
    "    X = toy_dataset.X\n",
    "    y = toy_dataset.y\n",
    "    assert toy_dataset.__len__() == N * N, f\"Wrong dataset size: {toy_dataset.__len__()}\"\n",
    "    assert len(X) == N * N, f\"Wrong X size: {X}\"\n",
    "    assert len(y) == N * N, f\"Wrong y size: {y}\"\n",
    "test_toy_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fiting the toy dataset\n",
    "#### ExU vs. standard ReLU\n",
    "Training prediction learned by a single hidden layer heural network with 1024 (a) standard ReLU, and (b) ReLU-n with ExU hidden units on the binary classfication toy dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(device='cpu', seed=2021, data_path='data/GALLUP.csv', experiment_name='NAM', regression=False, num_epochs=1000, lr=0.01, batch_size=128, logdir='output', wandb=False, hidden_sizes=[], activation='exu', dropout=0.0, feature_dropout=0.1, decay_rate=0.995, l2_regularization=0.1, output_regularization=0.1, num_basis_functions=1000, units_multiplier=2, shuffle=True, cross_val=False, num_folds=5, num_splits=3, fold_num=1, num_models=1, num_workers=16, save_model_frequency=2, save_top_k=3, use_dnn=False, early_stopping_patience=50)\n",
      "FeatureNN(\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (model): Sequential(\n",
      "    (0): ExU()\n",
      "    (1): Linear(in_features=1024, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "FeatureNN(\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (model): Sequential(\n",
      "    (0): LinearReLU()\n",
      "    (1): Linear(in_features=1024, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cfg = defaults()\n",
    "cfg.dropout = 0.0\n",
    "cfg.num_epochs = 1000\n",
    "cfg.wandb=False\n",
    "\n",
    "toy_dataset = ToyDataset(cfg, N=100, x_start=-1, x_end=1, p_start=0.1, p_end=0.9)\n",
    "\n",
    "# single hidden layer neural network with 1024 hidden units\n",
    "num_units = 1024\n",
    "cfg.hidden_sizes = [] \n",
    "print(cfg)\n",
    "\n",
    "relu_nn = FeatureNN(config=cfg, name=\"relu\", in_features=1, num_units=num_units, feature_index=0)\n",
    "cfg.activation = \"relu\"\n",
    "exu_nn = FeatureNN(config=cfg, name=\"exu\", in_features=1, num_units=num_units, feature_index=0)\n",
    "print(relu_nn)\n",
    "print(exu_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "from typing import Mapping\n",
    "from typing import Sequence\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "from tqdm.autonotebook import tqdm # progress bar \n",
    "\n",
    "from config import Config\n",
    "from models.saver import Checkpointer\n",
    "from trainer.losses import bce_loss\n",
    "from trainer.metrics import accuracy\n",
    "from trainer.metrics import mae\n",
    "from utils.loggers import TensorBoardLogger\n",
    "from ray import tune\n",
    "\n",
    "class NNTrainer:\n",
    "    def __init__(self, \n",
    "                 config: SimpleNamespace, \n",
    "                 model: nn.Module, \n",
    "                 dataset: torch.utils.data.Dataset):\n",
    "        \n",
    "        self.config = config\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=self.config.lr, weight_decay=self.config.decay_rate) \n",
    "        self.dataset = dataset\n",
    "        self.model = model \n",
    "\n",
    "        self.writer = TensorBoardLogger(config)\n",
    "        self.checkpointer = Checkpointer(model=model, config=config)\n",
    "\n",
    "        if config.wandb:\n",
    "            wandb.watch(models=self.model, log='all', log_freq=10)\n",
    "        \n",
    "        self.metrics_name = \"Accuracy\"\n",
    "        self.dataloader_train, self.dataloader_val, self.dataloader_test = self.dataset.get_dataloaders()\n",
    "        \n",
    "    \n",
    "    def train_step(\n",
    "        self, \n",
    "        batch: torch.Tensor, \n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a single mini-batch gredient-descient optimization step.\n",
    "        \"\"\"\n",
    "        features, targets = batch\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        preds = self.model(features)\n",
    "        loss = bce_loss(preds, targets)\n",
    "        metrics = accuracy(preds, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss, metrics\n",
    "    \n",
    "    def train_epoch(self, \n",
    "                    model: nn.Module, \n",
    "                    dataloader: torch.utils.data.DataLoader, \n",
    "        ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform an epoch of gradient descent optimization on dataloader\n",
    "        \"\"\"\n",
    "        model.train()\n",
    "        loss = 0.0\n",
    "        metrics = 0.0\n",
    "            \n",
    "        \n",
    "        with tqdm(dataloader, leave=False) as pbar:\n",
    "            for batch in pbar:\n",
    "                step_loss, step_metrics = self.train_step(batch)\n",
    "                loss += step_loss\n",
    "                metrics += step_metrics\n",
    "                    \n",
    "                pbar.set_description(f\"TL Step: {step_loss: .3f}|{self.metrics_name}:{step_metrics:.3f}\")\n",
    "                    \n",
    "        return loss / len(dataloader), metrics / len(dataloader)\n",
    "    \n",
    "    def evaluate_step(self, model: nn.Module, batch: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        features, targets = batch\n",
    "\n",
    "        # Forward pass from the model.\n",
    "        preds = model(features)\n",
    "\n",
    "        # Calculates loss on mini-batch.\n",
    "        loss = bce_loss(preds, targets)\n",
    "        metrics = accuracy(preds, targets)\n",
    "\n",
    "        # self.writer.write({\"val_loss_step\": loss.detach().cpu().numpy().item()})\n",
    "\n",
    "        return loss, metrics\n",
    "\n",
    "    def evaluate_epoch(self, model: nn.Module, dataloader: torch.utils.data.DataLoader) -> torch.Tensor:\n",
    "        \"\"\"Performs an evaluation of the `model` on the `dataloader.\"\"\"\n",
    "        model.eval()\n",
    "        loss = 0.0\n",
    "        metrics = 0.0\n",
    "        with tqdm(dataloader, leave=False) as pbar:\n",
    "            for batch in pbar:\n",
    "                # Accumulates loss in dataset.\n",
    "                with torch.no_grad():\n",
    "                    # step_loss = self.evaluate_step(model, batch, pbar)\n",
    "                    # loss += self.evaluate_step(model, batch, pbar)\n",
    "                    step_loss, step_metrics = self.evaluate_step(model, batch)\n",
    "                    loss += step_loss\n",
    "                    metrics += step_metrics\n",
    "\n",
    "                    pbar.set_description((f\"VL Step: {step_loss:.3f} | {self.metrics_name}: {step_metrics:.3f}\"))\n",
    "\n",
    "        return loss / len(dataloader), metrics / len(dataloader)\n",
    "    \n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        train models with mini-batch\n",
    "        \"\"\"\n",
    "        num_epochs = self.config.num_epochs\n",
    "        \n",
    "        with tqdm(range(num_epochs)) as pbar_epoch:\n",
    "            for epoch in pbar_epoch:\n",
    "                # trains model on whole training dataset\n",
    "                loss_train, metrics_train = self.train_epoch(self.model, self.dataloader_train)\n",
    "                # writes on TensorBoard\n",
    "                self.writer.write({\n",
    "                    \"loss_train_epoch\": loss_train.detach().cpu().numpy().item(),\n",
    "                    f\"{self.metrics_name}_train_epoch\": metrics_train,\n",
    "                })\n",
    "\n",
    "                # Evaluates model on whole validation dataset, and writes on `TensorBoard`.\n",
    "                loss_val, metrics_val = self.evaluate_epoch(self.model, self.dataloader_val)\n",
    "                self.writer.write({\n",
    "                    \"loss_val_epoch\": loss_val.detach().cpu().numpy().item(),\n",
    "                    f\"{self.metrics_name}_val_epoch\": metrics_val,\n",
    "                })\n",
    "\n",
    "                # Checkpoint model weights.\n",
    "                if epoch % self.config.save_model_frequency == 0:\n",
    "                    self.checkpointer.save(epoch)\n",
    "\n",
    "                tune.report(loss=loss_val,)\n",
    "\n",
    "                # Updates progress bar description.\n",
    "                pbar_epoch.set_description(f\"\"\"Epoch({epoch}):\n",
    "            TL: {loss_train.detach().cpu().numpy().item():.3f} |\n",
    "            VL: {loss_val.detach().cpu().numpy().item():.3f} |\n",
    "            {self.metrics_name}: {metrics_train:.3f}\"\"\")\n",
    "                \n",
    "    def test(self):\n",
    "        \"\"\"\n",
    "        test models with mini-batch \n",
    "        \"\"\"\n",
    "        num_epochs = self.config.num_epochs\n",
    "\n",
    "        for epoch in enumerate(range(num_epochs)):\n",
    "        # with tqdm(range(num_epochs)) as pbar_epoch:\n",
    "          #   for epoch in pbar_epoch:\n",
    "                # Evaluates model on whole validation dataset, and writes on `TensorBoard`.\n",
    "                loss_test, metrics_test = self.evaluate_epoch(self.model, self.dataloader_test)\n",
    "                print(f\"loss_test_epoch: {loss_test.detach().cpu().numpy().item()}, {self.metrics_name}_test_epoch: {metrics_test}\")\n",
    "                # tune.report(loss_test=loss_test.detach().cpu().numpy().item())\n",
    "                # self.writer.write({\n",
    "                  #   \"loss_test_epoch\": loss_test.detach().cpu().numpy().item(),\n",
    "                  #   f\"{self.metrics_name}_test_epoch\": metrics_test,\n",
    "                # })\n",
    "\n",
    "                # Updates progress bar description.\n",
    "                # pbar_epoch.set_description(\"Test Loss: {:.2f} \".format(loss_test.detach().cpu().numpy().item()))\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-19 08:41:12.243691: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "exu_trainer = NNTrainer(cfg, exu_nn, toy_dataset)\n",
    "relu_trainer = NNTrainer(cfg, relu_nn, toy_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir=\".output/logs\" --host \"0.0.0.0\" --port 6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu_trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NAMs learn a linear combination of neural networks that each attend to a single input feature; each sub net could have different number of ExU units. The outputs are added up (with a scalar bias), then passed through a link function for prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NAM(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        config, \n",
    "        name: str, \n",
    "        in_features: int,\n",
    "        num_units, \n",
    "        ) -> None: # type-check\n",
    "            \"\"\"\n",
    "            \n",
    "            Args:\n",
    "            in_features: size of each input sample \n",
    "            num_units: int(same number of units for all featurenns)/list. number of ExU units for *each* feature nn, which could be different. \n",
    "            \"\"\"\n",
    "            super(NAM, self).__init__()\n",
    "            self.config = config\n",
    "            # a feature NN for each feature\n",
    "            self.in_features = in_features\n",
    "            self.num_units = num_units\n",
    "            self.dropout = nn.Dropout(p=self.config.dropout)\n",
    "            \n",
    "            # num units for each feature neural net\n",
    "            if isinstance(self.num_units, list):\n",
    "                assert len(num_units) == in_features\n",
    "            elif isinstance(self.num_units, int):\n",
    "                self.num_units = [num_units for _ in range(self.in_features)]\n",
    "                \n",
    "            self.feature_nns = nn.ModuleList([\n",
    "                FeatureNN(self.config, \n",
    "                          name=f\"FeatureNN_{feature_index}\", \n",
    "                          in_features=1, \n",
    "                          num_units=self.num_units[feature_index], \n",
    "                          feature_index=feature_index) # note the in_features shape \n",
    "                for feature_index in range(self.in_features)\n",
    "            ])\n",
    "            self.bias = nn.Parameter(data=torch.zeros(1)) # bias initialized with 0; of shape (1)\n",
    "            \n",
    "    def features_output(self, inputs: torch.Tensor) -> Sequence[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Return list [torch.Tensor of shape (batch_size, 1)]: the output of each feature neural net\n",
    "        \"\"\"\n",
    "        return [self.feature_nns[feature_index](inputs[:, feature_index]) for feature_index in range(self.in_features)] # feature of shape (1, batch_size)\n",
    "            \n",
    "    def forward(self, inputs) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        inputs of shape (batch_size, in_features): input samples, \n",
    "        \n",
    "        Returns: \n",
    "        nam output of shape (batch_size): sum up the outputs of feature nets and bias\n",
    "        fnn outputs of shape (batch_size, in_features): output of each feature net\n",
    "        \"\"\"\n",
    "        # strong regularization \n",
    "        nn_outputs = self.features_output(inputs) # list [Tensor(batch_size,  1)]\n",
    "        cat_outputs = torch.cat(nn_outputs, dim=-1) # of shape (batch_size, in_features)\n",
    "        # regularization 2: feature dropout\n",
    "        dropout_outputs = self.dropout(cat_outputs) \n",
    "        outputs = dropout_outputs.sum(dim=-1) # sum along the features => of shape (batch_size)\n",
    "        return outputs + self.bias, dropout_outputs # note that outputs + bias => broadcast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "### DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        config, \n",
    "        name: str = \"DNNModel\",\n",
    "        in_features: int = 1, \n",
    "        out_features: int = 1, \n",
    "        ) -> None: # type-check\n",
    "            \"\"\"\n",
    "            DNN model as a baseline.\n",
    "            Args:\n",
    "            name: identifier for feature net selection\n",
    "            in_features: size of each input sample\n",
    "            out_features: size of each output sample\n",
    "\n",
    "            \"\"\"\n",
    "            super(DNN, self).__init__()\n",
    "            self.config = config\n",
    "            hidden_sizes = self.config.hidden_sizes\n",
    "            self.dropout = nn.Dropout(p=self.config.dropout)\n",
    "            layers = []\n",
    "            \n",
    "            layers.append(nn.Linear(in_features, hidden_sizes[0],  bias=True)) # with bias\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(self.dropout) # dropout \n",
    "            \n",
    "            for in_f, out_f in zip(hidden_sizes[:], hidden_sizes[1:]):\n",
    "                layers.append(nn.Linear(in_f, out_f,  bias=True))\n",
    "                layers.append(nn.ReLU())\n",
    "                layers.append(self.dropout)\n",
    "                \n",
    "            layers.append(nn.Linear(hidden_sizes[-1], out_features,  bias=True))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(self.dropout)\n",
    "            \n",
    "            self.model = nn.Sequential(*layers)\n",
    "            # TODO: Xavier initialization \n",
    "            self.apply(self.initialize_parameters) # note: apply function will recursively applies fn to every submodule\n",
    "    \n",
    "    def initialize_parameters(m):\n",
    "        # Xavier initlization \n",
    "        if isinstance(m, nn.Linear):\n",
    "        # if type(m) == nn.Linear: \n",
    "            # note that xavier initialization is mentioned in the article, but kaiming initlization is adopted here\n",
    "            # TODO: normal or uniform? why\n",
    "            torch.nn.init.kaiming_normal_(m.weight)\n",
    "            # torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "    def forward(self, inputs) -> torch.Tensor:\n",
    "        return self.model(inputs), None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### losses\n",
    "We train NAMs using the loss $L(\\theta)$ given by \n",
    "$$\n",
    "L(\\theta) = \\mathbb{E} _{x, y \\sim D}[l(x, y;\\theta) + \\lambda_1 \\eta (x; \\theta)] + \\lambda_2 \\gamma (\\theta) \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 2.0898],\n",
      "        [-1.9344],\n",
      "        [-1.8880],\n",
      "        [ 0.6682],\n",
      "        [-0.1440]]), tensor([[-0.0774],\n",
      "        [ 0.5431],\n",
      "        [ 0.1993],\n",
      "        [-0.4814],\n",
      "        [ 0.6061]]), tensor([[ 0.8519],\n",
      "        [ 0.4756],\n",
      "        [ 0.2438],\n",
      "        [ 0.2094],\n",
      "        [-1.0873]])]\n",
      "torch.Size([5, 3])\n",
      "torch.Size([5, 3])\n",
      "tensor([ 3.1826, -1.0174, -1.6055, -0.3021, -1.3680])\n",
      "tensor([ 4.1826, -0.0174, -0.6055,  0.6979, -0.3680])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "fnn_num = 3\n",
    "nn_outputs = [torch.randn(5, 1) for i in range(fnn_num)]\n",
    "print(nn_outputs)\n",
    "cat_outputs = torch.cat(nn_outputs, dim=-1)\n",
    "print(cat_outputs.shape)\n",
    "\n",
    "dropout = nn.Dropout(p=0.1)\n",
    "bias = torch.ones(1)\n",
    "\n",
    "dropout_out = dropout(cat_outputs)\n",
    "print(dropout_out.shape)\n",
    "out = dropout_out.sum(dim=-1)\n",
    "print(out)\n",
    "out = out + bias\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1938)\n",
      "tensor(1.1938)\n"
     ]
    }
   ],
   "source": [
    "K = len(dropout_out) # number of feature nets\n",
    "r1 = torch.mean(torch.square(dropout_out), 1).sum() / K\n",
    "per_feature_norm = [  # L2 Regularization\n",
    "            torch.mean(torch.square(outputs)) for outputs in dropout_out\n",
    "]\n",
    "r2 = sum(per_feature_norm) / len(per_feature_norm)\n",
    "print(r1)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(\n",
    "    logits: torch.Tensor, \n",
    "    targets: torch.Tensor, \n",
    ")-> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Mean squared error loss for regression \n",
    "    \"\"\"\n",
    "    return F.mse_loss(logits.view(-1), targets.view(-1))\n",
    "\n",
    "def bce_loss(\n",
    "    logits: torch.Tensor, \n",
    "    targets: torch.Tensor, \n",
    ")-> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Binary cross entropy loss for classification\n",
    "    \n",
    "    Args:\n",
    "    logits of shape (batch_size)\n",
    "    targets of shape (batch_size, 1)\n",
    "    \"\"\"\n",
    "    # note that we use bce instead of ce\n",
    "    # return F.cross_entropy(logits, targets)\n",
    "    return F.binary_cross_entropy_with_logits(logits.view(-1), targets.view(-1)) # convert to 1d vector \n",
    "    \n",
    "def penalized_loss(\n",
    "    config, \n",
    "    nam_out: torch.Tensor, \n",
    "    fnn_out: torch.Tensor, \n",
    "    model: nn.Module, \n",
    "    targets: torch.Tensor, \n",
    ")-> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute penalized loss of NAM\n",
    "    \n",
    "    Args:\n",
    "    nam_out of shape (batch_size): model output \n",
    "    fnn_out of shape (batch_size, in_features): output of each feature nn\n",
    "    model: the model that we use\n",
    "    targets of shape (batch_size): targets of each sample \n",
    "    \"\"\"\n",
    "    def fnn_loss(\n",
    "        fnn_out: torch.Tensor\n",
    "    )->torch.Tensor:\n",
    "        \"\"\"\n",
    "        Penalizes the L2 norm of the prediction of each feautre net\n",
    "        \n",
    "        Args: \n",
    "        fnn_out of shape (batch_size, in_features): output of each featrue nn\n",
    "        \"\"\"\n",
    "        num_fnn = len(fnn_out) # number of feature nets\n",
    "        return torch.mean(torch.square(fnn_out), 1).sum() / num_fnn\n",
    "        \n",
    "    def weight_decay(\n",
    "        model: nn.Module \n",
    "    )->torch.Tensor:\n",
    "        \"\"\"\n",
    "        Penalizes the L2 norm of weights in each *feature net*\n",
    "        \n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        if config.use_dnn:\n",
    "            num_networks = 1 \n",
    "        else:\n",
    "            num_networks = len(model.feature_nns)\n",
    "        l2_losses = [(p**2).sum() for p in model.parameters()]\n",
    "        return sum(l2_losses) / num_networks\n",
    "        \n",
    "    output_regularization = config.output_regularization\n",
    "    l2_regularization = config.l2_regularization\n",
    "    \n",
    "    loss = 0.0\n",
    "    # task dependent function \n",
    "    # TODO: not going through a logistic function?\n",
    "    if config.regression: \n",
    "        loss += mse_loss(nam_out, targets)\n",
    "    else:\n",
    "        loss += bce_loss(nam_out, targets)\n",
    "        \n",
    "    if output_regularization > 0:\n",
    "        loss += output_regularization * fnn_loss(fnn_out) # output penalty\n",
    "        \n",
    "    if l2_regularization > 0:\n",
    "        loss += l2_regularization * weight_decay(model) # weight decay \n",
    "        \n",
    "    return loss\n",
    "    # l = F.cross_entropy(out, targets)\n",
    "    # F.mse_loss(outputs, targets)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### metrics\n",
    "To evaluate the performance, we use AUC for binary classfication, and root mean-squared error (RMSE) for regression. \n",
    "\n",
    "=> mae for regression and acc for classification are actually used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(\n",
    "    logits: torch.Tensor, \n",
    "    targets: torch.Tensor\n",
    ")->torch.Tensor:\n",
    "    \"\"\"\n",
    "    Root mean-squared error for regression \n",
    "    \"\"\"\n",
    "    criterion = nn.MSELoss()\n",
    "    loss = torch.sqrt(criterion(logits.view(-1), targets.view(-1)))\n",
    "    return loss\n",
    "    \n",
    "def mae(\n",
    "    logits: torch.Tensor, \n",
    "    targets: torch.Tensor\n",
    ")->torch.Tensor:\n",
    "    \"\"\"\n",
    "    Mean absolute error \n",
    "    \"\"\"\n",
    "    return (((logits.view(-1) - targets.view(-1)).abs()).sum() / targets.numel()).item()\n",
    "\n",
    "def accuracy(\n",
    "    logits: torch.Tensor, \n",
    "    targets: torch.Tensor\n",
    ")-> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Accuracy for classification\n",
    "    \"\"\"\n",
    "    return (((targets.view(-1) > 0) == (logits.view(-1) > 0.5)).sum() / targets.numel()).item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from tqdm.autonotebook import tqdm\n",
    "from nam.models.saver import Checkpointer\n",
    "from nam.utils.loggers import TensorBoardLogger\n",
    "from ray import tune\n",
    "\n",
    "class Trainer: \n",
    "    def __init__(self, \n",
    "                config: SimpleNamespace, \n",
    "                 model: Sequence[nn.Module], \n",
    "                 dataset: torch.utils.data.Dataset\n",
    "                ) -> None:\n",
    "        self.config = Config(**vars(config)) # modification can be made to the default Config\n",
    "        self.optimizer = torch.optim.Adam(net.parameters(), lr=self.config.lr, weight_decay=self.config.decay_rate) \n",
    "        self.dataset = dataset\n",
    "        self.model = model\n",
    "        \n",
    "        self.writer = TensorBoardLogger(config)\n",
    "        self.checkpointer = Checkpointer(model=model, config=config)\n",
    "        \n",
    "        if config.wandb:\n",
    "            wandb.watch(models=self.model, log='all', log_freq=10)\n",
    "\n",
    "        \n",
    "        self.criterion = lambda nam_out, fnn_out, targets: penalized_loss(self.config, nam_out, fnn_out, self.model, targets)\n",
    "        self.metrics = lambda nam_out, targets: mae(nam_out, targets) if config.regression else accuracy(nam_out, targets)\n",
    "        self.metrics_name = \"MAE\" if config.regression else \"Accuracy\"\n",
    "        \n",
    "        self.dataloader_train, self.dataloader_val = self.dataset.train_dataloaders()\n",
    "        self.dataloader_test = self.dataset.test_dataloaders()\n",
    "        \n",
    "    def train_step(\n",
    "        self, \n",
    "        batch: torch.Tensor, \n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a single mini-batch gredient-descient optimization step.\n",
    "        \"\"\"\n",
    "        features, targets = batch\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        preds, fnn_out = self.model(features)\n",
    "\n",
    "        loss = self.criterion(preds, fnn_out, targets)\n",
    "        metrics = self.metrics(preds, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss, metrics\n",
    "    \n",
    "\n",
    "    def train_epoch(\n",
    "            self, \n",
    "            dataloader: torch.utils.data.DataLoader\n",
    "        ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform an epoch of gradient descent optimization on dataloader\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        loss = 0.0\n",
    "        metrics = 0.0\n",
    "            \n",
    "        with tqdm(dataloader, leave=False) as pbar:\n",
    "            for batch in pbar:\n",
    "                step_loss, step_metrics = self.train_step(batch)\n",
    "                loss += step_loss\n",
    "                metrics += step_metrics\n",
    "                    \n",
    "                pbar.set_description(f\"TL Step: {step_loss: .3f}|{self.metrics_name}:{step_metrics:.3f}\")\n",
    "                    \n",
    "        return loss / len(dataloader), metrics / len(dataloader)\n",
    "           \n",
    "\n",
    "    def evaluate_step(self, batch: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        features, targets = batch\n",
    "\n",
    "        # Forward pass from the model.\n",
    "        predictions, fnn_out = self.model(features)\n",
    "\n",
    "        # Calculates loss on mini-batch.\n",
    "        loss = self.criterion(predictions, fnn_out, targets)\n",
    "        metrics = self.metrics(predictions, targets)\n",
    "\n",
    "        # self.writer.write({\"val_loss_step\": loss.detach().cpu().numpy().item()})\n",
    "\n",
    "        return loss, metrics\n",
    "\n",
    "    def evaluate_epoch(self, dataloader: torch.utils.data.DataLoader) -> torch.Tensor:\n",
    "        \"\"\"Performs an evaluation of the `model` on the `dataloader.\"\"\"\n",
    "        self.model.eval()\n",
    "        loss = 0.0\n",
    "        metrics = 0.0\n",
    "        with tqdm(dataloader, leave=False) as pbar:\n",
    "            for batch in pbar:\n",
    "                # Accumulates loss in dataset.\n",
    "                with torch.no_grad():\n",
    "                    # step_loss = self.evaluate_step(model, batch, pbar)\n",
    "                    # loss += self.evaluate_step(model, batch, pbar)\n",
    "                    step_loss, step_metrics = self.evaluate_step(self.model, batch)\n",
    "                    loss += step_loss\n",
    "                    metrics += step_metrics\n",
    "\n",
    "                    pbar.set_description((f\"VL Step: {step_loss:.3f} | {self.metrics_name}: {step_metrics:.3f}\"))\n",
    "\n",
    "        return loss / len(dataloader), metrics / len(dataloader)\n",
    "    \n",
    "\n",
    "    def train(self):\n",
    "        num_epochs = self.config.num_epochs\n",
    "        \n",
    "        with tqdm(range(num_epochs)) as pbar_epoch:\n",
    "            for epoch in pbar_epoch:\n",
    "                loss_train, metrics_train = self.train_epoch(self.model, self.optimizer, self.dataloader_train)\n",
    "                # writes on TensorBoard\n",
    "         \n",
    "                self.writer.write({\n",
    "                    \"loss_train_epoch\": loss_train.detach().cpu().numpy().item(),\n",
    "                    f\"{self.metrics_name}_train_epoch\": metrics_train,\n",
    "                })\n",
    "\n",
    "                # Evaluates model on whole validation dataset, and writes on `TensorBoard`.\n",
    "                loss_val, metrics_val = self.evaluate_epoch(self.model, self.dataloader_val)\n",
    "                self.writer.write({\n",
    "                    \"loss_val_epoch\": loss_val.detach().cpu().numpy().item(),\n",
    "                    f\"{self.metrics_name}_val_epoch\": metrics_val,\n",
    "                })\n",
    "\n",
    "                # Checkpoints model weights.\n",
    "                if epoch % self.config.save_model_frequency == 0:\n",
    "                    self.checkpointer.save(epoch)\n",
    "\n",
    "                tune.report(loss=loss_val,)\n",
    "\n",
    "                # Updates progress bar description.\n",
    "                pbar_epoch.set_description(f\"\"\"Epoch({epoch}):\n",
    "            TL: {loss_train.detach().cpu().numpy().item():.3f} |\n",
    "            VL: {loss_val.detach().cpu().numpy().item():.3f} |\n",
    "            {self.metrics_name}: {metrics_train:.3f}\"\"\")\n",
    "                \n",
    "    def test(self):\n",
    "        num_epochs = self.config.num_epochs\n",
    "\n",
    "        with tqdm(range(num_epochs)) as pbar_epoch:\n",
    "            for epoch in pbar_epoch:\n",
    "\n",
    "                # Evaluates model on whole validation dataset, and writes on `TensorBoard`.\n",
    "                loss_test, metrics_test = self.evaluate_epoch(self.model, self.dataloader_test)\n",
    "                # tune.report(loss_test=loss_test.detach().cpu().numpy().item())\n",
    "                self.writer.write({\n",
    "                    \"loss_test_epoch\": loss_test.detach().cpu().numpy().item(),\n",
    "                    f\"{self.metrics_name}_test_epoch\": metrics_test,\n",
    "                })\n",
    "\n",
    "                # Updates progress bar description.\n",
    "                pbar_epoch.set_description(\"Test Loss: {:.2f} \".format(loss_test.detach().cpu().numpy().item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (module anaconda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
