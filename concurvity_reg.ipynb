{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LANAM.models import  NAM, LaNAM\n",
    "from LANAM.trainer import *\n",
    "from LANAM.trainer.nam_trainer import train\n",
    "from LANAM.trainer import test\n",
    "from LANAM.config.default import * \n",
    "from LANAM.data import *\n",
    "from LANAM.data.base import LANAMDataset, LANAMSyntheticDataset\n",
    "from LANAM.utils.plotting import * \n",
    "from LANAM.utils.hsic import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import copy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concurvity Regularization\n",
    "## Preliminary\n",
    "### NAM \n",
    "$$\n",
    "h(y) = f_1(x_1) + \\cdot + f_d(x_d) + \\beta_0 \n",
    "$$\n",
    "where $\\beta_0$ is the global bias.\n",
    "### Concurvity\n",
    "1. Why we don't like concurvity: </br>\n",
    "fitted model becomes less interpretable as each feature's contribution to the target is not immediately apparently.\n",
    "2. Target: pairwise uncorrelatedness, $\\text{corr}(f_i, f_j) = 0$</br>\n",
    "where $\\text{corr}(\\cdot)$ is the Pearson correlation coefficient: \n",
    "$$ \n",
    "r_{xy} = \\frac{\\sum_i(x-\\overline x)(y - \\overline y)}{\\sqrt{\\sum_i (x-\\overline x)^2}\\sqrt{\\sum_i (y-\\overline y)^2}}\n",
    "$$\n",
    "\n",
    "3. Method: concurvity regularization $\\frac{1}{p(p-1)/2}\\sum_{i=1}^p \\sum_{j=i+1}^p \\left|\\text{corr}\\left(f_i(X_i), f_j(X_j)\\right)\\right|$\n",
    "\n",
    "4. Evaluation: three different strategies are used for evaluation, \n",
    "    - Pairwise correlation: $\\text{corr}(f_i, f_j)$\n",
    "    - Correlation between target and transformed features: $\\text{corr}(f_i, y)$\n",
    "    - Estimated feature importance (sensitivity): variance of shape function on training set. \n",
    "        - $\\text{FI}_i[f_i(x_i)] = \\frac{1}{N}\\sum^N_{j}|f_i(x_{ij}) - \\overline{f_i}|$ for transformed features, where $\\overline{f}$Â is the mean value of shape function on training data.  \n",
    "        - $\\text{FI}_i[x_i] = \\frac{1}{N}\\sum^N_{j}|x_{ij} - \\overline{x_i}|$ for untransformed features. \n",
    "   \n",
    "5. Performance and concurvity trade-off.\n",
    "\n",
    "## Questions\n",
    "1. Feature importance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About toy examples\n",
    "1. Experimental setup sharing between synthetic examples: \n",
    "    - $10000$ samples, dataset split: 7: 2: 1.\n",
    "    - activation function: GELU\n",
    "    - three hidden layers, each of which contains $128$ units.</br>\n",
    "    - concurvity regularization parameter $\\lambda \\in [1e-6, 1]$. (fig. 3(b))\n",
    "    \n",
    "2. Questions: \n",
    "    - isn't the training dataset too large? => $10000 \\rightarrow 1000$, no impact. \n",
    "    - different activation functions => \n",
    "    - different hidden layers => single layer: requires more training epochs.\n",
    "    - behaviours: \n",
    "        - strongly correlated features are **all** muted. \n",
    "            - when strongly correlated features are important? performance-concurvity trade-off. \n",
    "        - mostly uncorrelated features remain.\n",
    "    - training samples are shuffled; appoximate the global distribution; global correlation. \n",
    "\n",
    "3. Comments: \n",
    "    - sampling on subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex2\n",
    "#### Ex2.0\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&Y = 0 \\cdot X_1 + 1 \\cdot X_2, \\\\\n",
    "&X_1 =  Z, \\\\\n",
    "&X_2 = |Z|, \\\\\n",
    "&Z \\sim \\mathcal{N}(0, 1), \\quad \\text{truncated by } (-1, 1)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonlinearly_dependent_data = load_nonlinearly_dependent_2D_examples(num_samples=1000) # uncorrelated features \n",
    "nd_train_dl, _, nd_val_dl, _ = nonlinearly_dependent_data.train_dataloaders()\n",
    "nd_test_samples = nonlinearly_dependent_data.get_test_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonlinearly_dependent_data.plot_dataset()\n",
    "untransformed_nd_feature_importance = feature_importance(nonlinearly_dependent_data.features)\n",
    "untransformed_nd_feature_correlation = pairwise_correlation(nonlinearly_dependent_data.features)\n",
    "print(f'[nonlinearly dependent dataset]: untransformed feature importance: {untransformed_nd_feature_importance}')\n",
    "print(f'[nonlinearly dependent dataset]: corr(X1, X2): {untransformed_nd_feature_correlation[0][1]: .6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = toy_default()\n",
    "cfg.log_loss_frequency = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.concurvity_regularization = 0\n",
    "nd_wo_model = train(config=cfg, train_loader=nd_train_dl, val_loader=nd_val_dl, test_samples=nd_test_samples, ensemble=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, shape_functions, names = nd_test_samples\n",
    "    \n",
    "prediction_mean, feature_contribution_mean, prediction_mean, feature_contribution_var = get_prediction(nd_wo_model, nd_test_samples)\n",
    "\n",
    "plt.scatter(feature_contribution_mean[:, 0], feature_contribution_mean[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.concurvity_regularization = 0.1\n",
    "nd_w_model = train(config=cfg, train_loader=nd_train_dl, val_loader=nd_val_dl, test_samples=nd_test_samples, ensemble=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ex2.2\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&Y = 0 \\cdot X_1 + 1 \\cdot X_2, \\\\\n",
    "&X_1 =  Z, \\\\\n",
    "&X_2 = \\sin(4Z), \\\\\n",
    "&Z \\sim \\mathcal{N}(0, 1), \\quad \\text{truncated by } (-1, 1)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_nonlinearly_dependent_2D_examples(num_samples=1000, sampling_type='normal', \n",
    "                                                                    dependent_functions=lambda x: torch.sin(3*x)) # uncorrelated features \n",
    "train_dl, _, val_dl, _ = data.train_dataloaders()\n",
    "test_samples = data.get_test_samples()\n",
    "\n",
    "data.plot_dataset()\n",
    "fig, axs = plt.subplots(figsize=(4,3))\n",
    "axs.set_title('Relation between untransformed features')\n",
    "axs.set_xlabel('X2')\n",
    "axs.set_ylabel('X1')\n",
    "axs.scatter(data.features[:, 0], data.features[:, 1])\n",
    "untransformed_nd_feature_importance = feature_importance(data.features)\n",
    "untransformed_nd_feature_correlation = pairwise_correlation(data.features)\n",
    "print(f'[nonlinearly dependent dataset]: untransformed feature importance: {untransformed_nd_feature_importance}')\n",
    "print(f'[nonlinearly dependent dataset]: corr(X1, X2): {untransformed_nd_feature_correlation[0][1]: .6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = toy_default()\n",
    "cfg.num_epochs = 400\n",
    "cfg.log_loss_frequency = 100\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.concurvity_regularization = 0\n",
    "nd_wo_model = train(config=cfg, train_loader=train_dl, val_loader=val_dl, test_samples=test_samples, ensemble=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.concurvity_regularization = 0.1\n",
    "nd_wo_model = train(config=cfg, train_loader=train_dl, val_loader=val_dl, test_samples=test_samples, ensemble=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ex2.1\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&Y = 0 \\cdot X_1 + 1 \\cdot X_2, \\\\\n",
    "&X_1 =  Z, \\\\\n",
    "&X_2 = \\sin(2Z), \\\\\n",
    "&Z \\sim \\mathcal{N}(0, 1), \\quad \\text{truncated by } (-1, 1)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonlinearly_dependent_data = load_nonlinearly_dependent_2D_examples(num_samples=1000, dependent_functions=lambda x: torch.sin(2*x)) # uncorrelated features \n",
    "nd_train_dl, _, nd_val_dl, _ = nonlinearly_dependent_data.train_dataloaders()\n",
    "nd_test_samples = nonlinearly_dependent_data.get_test_samples()\n",
    "\n",
    "nonlinearly_dependent_data.plot_dataset()\n",
    "fig, axs = plt.subplots(figsize=(4,3))\n",
    "axs.set_title('Relation between untransformed features')\n",
    "axs.set_xlabel('X2')\n",
    "axs.set_ylabel('X1')\n",
    "axs.scatter(nonlinearly_dependent_data.features[:, 0], nonlinearly_dependent_data.features[:, 1])\n",
    "untransformed_nd_feature_importance = feature_importance(nonlinearly_dependent_data.features)\n",
    "untransformed_nd_feature_correlation = pairwise_correlation(nonlinearly_dependent_data.features)\n",
    "print(f'[nonlinearly dependent dataset]: untransformed feature importance: {untransformed_nd_feature_importance}')\n",
    "print(f'[nonlinearly dependent dataset]: corr(X1, X2): {untransformed_nd_feature_correlation[0][1]: .6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = toy_default()\n",
    "cfg.concurvity_regularization = 0.06\n",
    "nd_wo_model = train(config=cfg, train_loader=nd_train_dl, val_loader=nd_val_dl, test_samples=nd_test_samples, ensemble=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex3: concurvity examples\n",
    "$$\n",
    "X_1 \\sim X_2 \\sim X_3 \\sim U(0,1)\\\\\n",
    "X_4 = X_2^3 + X_3 ^ 2 + \\sigma_1\\\\\n",
    "X_5 = X_3^2+\\sigma_1\\\\\n",
    "X_6 = X_2^2 + X_4^3+\\sigma_1 \\\\\n",
    "X_7 = X_1 \\times X_4 +\\sigma_1\\\\\n",
    "Y = 2X_1^2 + X_5^3 + 2\\sin X_6+\\sigma_2\n",
    "$$\n",
    "#### Ex3.1: different hidden sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concurvity_data = load_concurvity_data(sigma_1=0.05, sigma_2=0.5, num_samples=1000)\n",
    "con_train_dl, con_train_dl_fnn, con_val_dl, _ = concurvity_data.train_dataloaders()\n",
    "con_test_samples = concurvity_data.get_test_samples()\n",
    "concurvity_data.plot_dataset()\n",
    "# concurvity_data.plot_scatterplot_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = defaults()\n",
    "lanam = LaNAM(config=cfg, name=\"LA-NAM\", in_features=concurvity_data.in_features, hessian_structure='kron', subset_of_weights='last_layer')\n",
    "\n",
    "lanam, margs, losses, perfs = marglik_training(lanam, \n",
    "                                               con_train_dl, \n",
    "                                               con_train_dl_fnn, \n",
    "                                               con_val_dl, \n",
    "                                               likelihood='regression', \n",
    "                                               test_samples=con_test_samples,\n",
    "                                               n_epochs=500, \n",
    "                                               use_wandb=False, \n",
    "                                               optimizer_kwargs={'lr': 1e-2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test, fnn_test, _ = con_test_samples\n",
    "f_mu, f_var, f_mu_fnn, f_var_fnn = lanam.predict(X_test)\n",
    "\n",
    "importance_fig = plot_feature_importance(lanam, con_test_samples)\n",
    "\n",
    "recover_fig = plot_recovered_functions(X_test, y_test, fnn_test, f_mu_fnn, f_var_fnn.flatten(start_dim=1), center=False)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = toy_default()\n",
    "# cfg.output_regularization = 0.05\n",
    "cfg.log_loss_frequency = 100\n",
    "cfg.concurvity_regularization = 0\n",
    "cfg.num_ensemble = 5\n",
    "cfg.num_epochs = 400\n",
    "cfg.early_stopping_patience = 40\n",
    "con_wo_model = train(config=cfg, train_loader=con_train_dl, val_loader=con_val_dl, test_samples=con_test_samples, ensemble=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = toy_default()\n",
    "# cfg.output_regularization = 0.05\n",
    "cfg.log_loss_frequency = 100\n",
    "cfg.concurvity_regularization = 0.5\n",
    "cfg.num_ensemble = 5\n",
    "cfg.num_epochs = 400\n",
    "cfg.early_stopping_patience = 40\n",
    "con_wo_model = train(config=cfg, train_loader=con_train_dl, val_loader=con_val_dl, test_samples=con_test_samples, ensemble=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = toy_default()\n",
    "# cfg.output_regularization = 0.05\n",
    "cfg.log_loss_frequency = 100\n",
    "cfg.concurvity_regularization = 0\n",
    "cfg.num_ensemble = 5\n",
    "cfg.num_epochs = 400\n",
    "cfg.early_stopping_patience = 20\n",
    "con_wo_model = train(config=cfg, train_loader=con_train_dl, val_loader=con_val_dl, test_samples=con_test_samples, ensemble=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex1: multicollinearity \n",
    "given linear model \n",
    "$$\n",
    "\\begin{aligned}\n",
    "Y = 1\\cdot X_1+0\\cdot X_2\n",
    "\\end{aligned}\n",
    "$$\n",
    "we generate feature $X_1$ and $X_2$ by sampling from a *uniform* distribution with two different settings: \n",
    "- independently sampled;\n",
    "- fixed to identical samples (perfectly correlated).\n",
    "\n",
    "Except for output penality, all the other regularization terms for the vanilla NAM are set as zeros. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex1.1: natural preference of NAM \n",
    "https://wandb.ai/xinyu-zhang/NAM_preference_multicolinearity?workspace=user-xinyu-zhang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex1.0\n",
    "#### build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_funcs =[lambda x: x, lambda x: torch.zeros_like(x)]\n",
    "\n",
    "uncorrelated_data = load_synthetic_data(generate_functions=generate_funcs, x_lims=(-1, 1), num_samples=1000, sigma=0, sampling_type='uniform') # uncorrelated features \n",
    "uc_train_dl, _, uc_val_dl, _ = uncorrelated_data.train_dataloaders()\n",
    "uc_test_samples = uncorrelated_data.get_test_samples()\n",
    "\n",
    "# generate perfectly correlated data\n",
    "perfect_correlated_data = load_multicollinearity_data(generate_functions=generate_funcs, x_lims=(-1, 1), num_samples=1000, sigma=0, sampling_type='uniform') # perfectly correlated features \n",
    "pc_train_dl, _, pc_val_dl, _ = perfect_correlated_data.train_dataloaders()\n",
    "pc_test_samples = perfect_correlated_data.get_test_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: when measuring accuracy and concurvity trade-off...\n",
    "DON'T use ensembling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ensemble models\n",
    "model = train(config=cfg, train_loader=pc_train_dl, val_loader=pc_val_dl, ensemble=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(8, 8))\n",
    "# testing\n",
    "pred, fnn = get_ensemble_prediction(model, pc_test_samples[0], pc_test_samples[1])\n",
    "f_mu, f_mu_fnn, f_var, f_var_fnn = pred.mean(dim=0), fnn.mean(dim=0), pred.var(dim=0), fnn.var(dim=0)\n",
    "r = concurvity_loss(f_mu_fnn)\n",
    "print(f'measured concurvity with ensembling: {r.item(): .4f}')\n",
    "fig.supxlabel('X')\n",
    "fig.supylabel('f(X)')\n",
    "\n",
    "axs[0][0].scatter(pc_test_samples[0][:, 0], f_mu_fnn[:, 0])\n",
    "axs[0][1].scatter(pc_test_samples[0][:, 1], f_mu_fnn[:, 1])\n",
    "\n",
    "individual_r = list()\n",
    "for idx in range(cfg.num_ensemble): \n",
    "    # individual \n",
    "    f_mu, f_mu_fnn = pred[idx, :], fnn[idx, : ]\n",
    "    axs[1][0].scatter(pc_test_samples[0][:, 0], f_mu_fnn[:, 0])\n",
    "    axs[1][1].scatter(pc_test_samples[0][:, 1], f_mu_fnn[:, 1])\n",
    "\n",
    "    r = concurvity_loss(f_mu_fnn)\n",
    "    print(f'measured concurvity for individual model_{idx}: {r.item(): .4f}')\n",
    "    individual_r.append(r.item())\n",
    "\n",
    "print(max(individual_r), min(individual_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncorrelated_data.plot_dataset()\n",
    "untransformed_uc_feature_correlation = pairwise_correlation(torch.concatenate([uncorrelated_data.features, uncorrelated_data.targets], dim=1))\n",
    "untransformed_uc_feature_importance = feature_importance(uncorrelated_data.features)\n",
    "print(f'[uncorrelated dataset]: untransformed feature importance: {untransformed_uc_feature_importance}')\n",
    "print(f'[uncorrelated dataset]: corr(X1, X2): {untransformed_uc_feature_correlation[0][1]: .6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perfect_correlated_data.plot_dataset()\n",
    "untransformed_pc_feature_importance = feature_importance(perfect_correlated_data.features)\n",
    "untransformed_pc_feature_correlation = pairwise_correlation(perfect_correlated_data.features)\n",
    "print(f'[perfectly correlated dataset]: untransformed feature importance: {untransformed_pc_feature_importance}')\n",
    "print(f'[perfectly correlated dataset]: corr(X1, X2): {untransformed_pc_feature_correlation[0][1]: .6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = toy_default() # configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### uncorrelated data, without and with concurvity regularization\n",
    "**Claim**: Page 13, 'concurvity regularizer R does not automaticalkly affect the predictive performance of a GAM'. \n",
    "\n",
    "**Experimental result**: \n",
    "1. the correlation of untransformed $X_1$ and $X_2$ ($corr(X_1, X_2)$): within $\\pm 0.01$. \n",
    "2. <span style='color: red'>with concurvity regularization parameter $\\lambda = 1$, Val. RMSE increase from $1e-5$ to $1e-3$. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.concurvity_regularization = 0 \n",
    "uc_wo_model = train(config=cfg, train_loader=uc_train_dl, val_loader=uc_val_dl, test_samples=uc_test_samples, ensemble=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.concurvity_regularization = 1\n",
    "uc_w_model = train(config=cfg, train_loader=uc_train_dl, val_loader=uc_val_dl, test_samples=uc_test_samples, ensemble=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### perfectly correlated data, with and without concurvity regularization\n",
    "number of ensemble members: $40$. \n",
    "\n",
    "**Claim**: decrease $corr(f_1(X_1), f_2(X_2)$.\n",
    "\n",
    "**Experimental result**:\n",
    "- overall correlation $corr(f_1(X_1), f_2(X_2)$ decrease from $1$ to $1e-3$. \n",
    "- <span style='color:red'>the model fails to recover individual functions.</span> Piecewise correlations are generated whose additive impacts counteract. \n",
    "    - identical features, no natural bias?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.num_ensemble = 40\n",
    "cfg.concurvity_regularization = 0 \n",
    "pc_wo_model = train(config=cfg, train_loader=pc_train_dl, val_loader=pc_val_dl, test_samples=pc_test_samples, ensemble=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.concurvity_regularization = 1\n",
    "pc_w_model = train(config=cfg, train_loader=pc_train_dl, val_loader=pc_val_dl, test_samples=pc_test_samples, ensemble=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular data \n",
    "### California housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "california_housing_data = load_sklearn_housing_data()\n",
    "cal_train_dl, _, cal_val_dl, _ = california_housing_data.train_dataloaders()\n",
    "cal_test_dl, _ = california_housing_data.test_dataloaders()\n",
    "print(f'number of features: {california_housing_data.in_features}, dataset size: {len(california_housing_data.features)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chcfg = toy_default()\n",
    "chcfg.decay_rate=3.73e-3  # necessary \n",
    "chcfg.activation_cls = 'relu'\n",
    "chcfg.hidden_sizes=[72, 72, 72, 72, 72]\n",
    "chcfg.num_epochs = 40\n",
    "chcfg.batch_size = 512\n",
    "chcfg.lr = 9.46e-3\n",
    "chcfg.log_loss_frequency = 1\n",
    "chcfg.concurvity_regularization = 0\n",
    "print(chcfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_model = train(config=chcfg, train_loader=cal_train_dl, val_loader=cal_val_dl, ensemble=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rmse = torch.sqrt(test('regression', cal_model[0], cal_test_dl))\n",
    "print(test_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_model[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (module anaconda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
